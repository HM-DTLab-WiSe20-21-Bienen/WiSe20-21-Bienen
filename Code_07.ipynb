{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifizierter YOLO Algorithmus\n",
    "Valentino Golob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aufbereitung der Daten\n",
    "- Hier sollte man noch eine geschickter Aufteilung wählen\n",
    "- Eine einfache Möglichkeit wäre hier sich an der Uhrzeit zu orientiern, setzt voraus dass die Lichtverhältnisse mit der Uhrzeit korrelieren  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(3)\n",
    "lines = open('PathImages.txt').readlines()\n",
    "random.shuffle(lines)\n",
    "open('PathImages_train.txt', 'w').writelines(lines[:1600])\n",
    "open('PathImages_test.txt', 'w').writelines(lines[1600:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Umwandlung von absoluten Bezügen in relativ normierten Bezugsystem\n",
    "Die Positionsbestimmung der Bounding Boxes (BB) in dem unbearbeiteten Datensatz erfolgt in absoluten Pixel Koordinaten. Um die Anwendung und Implementierung bestehender Algorithmen zu erleichtern, werden die Werte zur Bestimmung der lokalen Position und Größe der BB in einem Preprocessing Schritt in relative Koordinaten umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creates all the labels (bounding boxes) for the corresponding images\n",
    "all the values are normalized to values between 0 and 1\n",
    "example: \n",
    "    image name: 2020_06_26_06_02_33.png\n",
    "    label name: 2020_06_26_06_02_33.txt\n",
    "structure of the 2020_06_26_06_02_33.txt (label file):\n",
    "    - multiple rows for each bounding box\n",
    "    - 0 0.6807191568505889 0.21293800539083557 0.030998140111593304 0.16711590296495957\n",
    "    - class_id, x_center, y_center, width, height\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# open manifest data\n",
    "file_manifest = open('label.txt', 'r')\n",
    "\n",
    "count = 0\n",
    "for json_file in file_manifest:\n",
    "    dict = json.loads(json_file)\n",
    "    file_name = dict['source-ref'].split('/')[-1].replace('png','txt').rstrip()\n",
    "    image_depth = dict['bee-labeling-2k-batch-01']['image_size'][0]['depth']\n",
    "    image_width = dict['bee-labeling-2k-batch-01']['image_size'][0]['width']\n",
    "    image_height = dict['bee-labeling-2k-batch-01']['image_size'][0]['height']\n",
    "    image_human_annotated = dict['bee-labeling-2k-batch-01-metadata']['human-annotated'].rstrip()\n",
    "\n",
    "    if len(dict['bee-labeling-2k-batch-01']['annotations']) != 0:\n",
    "        class_id, xc, yc, top, left, height, width = ([] for _ in range(7))\n",
    "        for dict_item in dict['bee-labeling-2k-batch-01']['annotations']:\n",
    "            class_id.append(dict_item['class_id'])\n",
    "            top.append(dict_item['top'])\n",
    "            left.append(dict_item['left'])\n",
    "            height.append((dict_item['height'])/image_height)\n",
    "            width.append((dict_item['width'])/image_width)\n",
    "            xc.append((dict_item['left'] + (dict_item['width']/2))/image_width)\n",
    "            yc.append((dict_item['top'] + (dict_item['height']/2))/image_height)\n",
    "\n",
    "        path_file_name = os.path.join('../data/labels', file_name)\n",
    "        with open(path_file_name, 'w') as file_label:\n",
    "            for i in range(len(class_id)):\n",
    "                file_label.write(\"{} {} {} {} {}\\n\".format(int(class_id[i]), xc[i], yc[i], width[i], height[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### txt. Datei mit den Dateipfaden zu den Bildern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('../data/images')\n",
    "\n",
    "dirList = os.listdir(path)\n",
    "\n",
    "#f = open('PathImages_' + path_option + '.txt', 'w')\n",
    "f = open('PathImages.txt', 'w')\n",
    "for filename in dirList:\n",
    "    f.write('./data/images/' + filename + '\\n')\n",
    "    #f.write('./data_'+ path_option + '/images/' + filename + '\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import notwendiger Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creates all the labels (bounding boxes) for the corresponding images\n",
    "all the values are normalized to values between 0 and 1\n",
    "example: \n",
    "    image name: 2020_06_26_06_02_33.png\n",
    "    label name: 2020_06_26_06_02_33.txt\n",
    "structure of the 2020_06_26_06_02_33.txt (label file):\n",
    "    - multiple rows for each bounding box\n",
    "    - 0 0.6807191568505889 0.21293800539083557 0.030998140111593304 0.16711590296495957\n",
    "    - class_id, x_center, y_center, width, height\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# open manifest data\n",
    "file_manifest = open('label.txt', 'r')\n",
    "\n",
    "count = 0\n",
    "for json_file in file_manifest:\n",
    "    dict = json.loads(json_file)\n",
    "    file_name = dict['source-ref'].split('/')[-1].replace('png','txt').rstrip()\n",
    "    image_depth = dict['bee-labeling-2k-batch-01']['image_size'][0]['depth']\n",
    "    image_width = dict['bee-labeling-2k-batch-01']['image_size'][0]['width']\n",
    "    image_height = dict['bee-labeling-2k-batch-01']['image_size'][0]['height']\n",
    "    image_human_annotated = dict['bee-labeling-2k-batch-01-metadata']['human-annotated'].rstrip()\n",
    "\n",
    "    if len(dict['bee-labeling-2k-batch-01']['annotations']) != 0:\n",
    "        class_id, xc, yc, top, left, height, width = ([] for _ in range(7))\n",
    "        for dict_item in dict['bee-labeling-2k-batch-01']['annotations']:\n",
    "            class_id.append(dict_item['class_id'])\n",
    "            top.append(dict_item['top'])\n",
    "            left.append(dict_item['left'])\n",
    "            height.append((dict_item['height'])/image_height)\n",
    "            width.append((dict_item['width'])/image_width)\n",
    "            xc.append((dict_item['left'] + (dict_item['width']/2))/image_width)\n",
    "            yc.append((dict_item['top'] + (dict_item['height']/2))/image_height)\n",
    "\n",
    "        path_file_name = os.path.join('../data/labels', file_name)\n",
    "        with open(path_file_name, 'w') as file_label:\n",
    "            for i in range(len(class_id)):\n",
    "                file_label.write(\"{} {} {} {} {}\\n\".format(int(class_id[i]), xc[i], yc[i], width[i], height[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 8 # 64 in original paper but I don't have that much vram\n",
    "#WEIGHT_DECAY = 0\n",
    "EPOCHS = 500\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
    "SPLIT_SIZE = (12, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model - angepasst nach Quelle: [Aladdin Persson - YOLO/model.py](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs) # bias to false, it is already included in the batch norm\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "\n",
    "\n",
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, hidden_size_FFN, architecture_config, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.hidden_size_FFN = hidden_size_FFN\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        B, C = num_boxes, num_classes\n",
    "        S1, S2 = split_size\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * S1 * S2, self.hidden_size_FFN),\n",
    "            nn.Dropout(0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(self.hidden_size_FFN, S1 * S2 * (C + B * 5)),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss function - angepasst nach Quelle: [Aladdin Persson - YOLO/loss.py](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/loss.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Yolo Loss Function from the original yolo paper\n",
    "\"\"\"\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculate the loss for the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, S=(7,7), B=2, C=1):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "        \"\"\"\n",
    "        S is split size of image (in paper 7), in mod. Version S1 and S2\n",
    "        B is number of boxes (in paper 2),\n",
    "        C is number of classes, in Bee Project 1\n",
    "        \"\"\"\n",
    "        self.S1, self.S2 = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "        # These are from Yolo paper, signifying how much we should\n",
    "        # pay loss for no object (noobj) and the box coordinates (coord)\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        # predictions are shaped (BATCH_SIZE, S*S(C+B*5) when inputted\n",
    "        predictions = predictions.reshape(-1, self.S1, self.S2, self.C + self.B * 5)\n",
    "\n",
    "        # Calculate IoU for the two predicted bounding boxes with target bbox\n",
    "        iou_b1 = intersection_over_union(predictions[..., 2:6], target[..., 2:6])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 7:11], target[..., 2:6])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        # Take the box with highest IoU out of the two prediction\n",
    "        # Note that bestbox will be indices of 0, 1 for which bbox was best\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 1].unsqueeze(3)  # in paper this is Iobj_i\n",
    "\n",
    "        # ======================== #\n",
    "        #   FOR BOX COORDINATES    #\n",
    "        # ======================== #\n",
    "\n",
    "        # Set boxes with no object in them to 0. We only take out one of the two\n",
    "        # predictions, which is the one with highest Iou calculated previously.\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 7:11]\n",
    "                + (1 - bestbox) * predictions[..., 2:6]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 2:6]\n",
    "\n",
    "        # Take sqrt of width, height of boxes to ensure that\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
    "        )\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # ==================== #\n",
    "        #   FOR OBJECT LOSS    #\n",
    "        # ==================== #\n",
    "\n",
    "        # pred_box is the confidence score for the bbox with highest IoU\n",
    "        pred_box = (\n",
    "            bestbox * predictions[..., 6:7] + (1 - bestbox) * predictions[..., 1:2]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 1:2]),\n",
    "        )\n",
    "\n",
    "        # ======================= #\n",
    "        #   FOR NO OBJECT LOSS    #\n",
    "        # ======================= #\n",
    "\n",
    "        #max_no_obj = torch.max(predictions[..., 20:21], predictions[..., 25:26])\n",
    "        #no_object_loss = self.mse(\n",
    "        #    torch.flatten((1 - exists_box) * max_no_obj, start_dim=1),\n",
    "        #    torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
    "        #)\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 1:2], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 1:2], start_dim=1),\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 6:7], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 1:2], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # ================== #\n",
    "        #   FOR CLASS LOSS   #\n",
    "        # ================== #\n",
    "\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :1], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :1], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss  # first two rows in paper\n",
    "            + object_loss  # third row in paper\n",
    "            + self.lambda_noobj * no_object_loss  # forth row\n",
    "            + class_loss  # fifth row\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bienen Datensatz - Code teilweise aus PyTorch Computer Vision Cookbook - Kapitel [Single-Object-Detection](https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook/blob/master/Chapter04/Ch4_Scirpts.ipynb) und Kapitel [Multi-Object-Detection](https://github.com/PacktPublishing/PyTorch-Computer-Vision-Cookbook/blob/master/Chapter05/Chapter%205.ipynb) und [Aladdin Persson - YOLO/dataset.py](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeeDataset(Dataset):\n",
    "    def __init__(self, path2listFile, S=(7,7), B=2, C=1, transform=None, trans_params=None):\n",
    "        with open(path2listFile, \"r\") as file:\n",
    "            self.path2imgs = file.readlines()\n",
    "\n",
    "        self.path2labels = [\n",
    "            path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\")\n",
    "            for path in self.path2imgs]\n",
    "\n",
    "        self.trans_params = trans_params\n",
    "        self.transform = transform\n",
    "        self.S1, self.S2 = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path2imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path2img = self.path2imgs[index].rstrip()\n",
    "\n",
    "        img = Image.open(path2img).convert('RGB')\n",
    "\n",
    "        path2label = self.path2labels[index].rstrip()\n",
    "\n",
    "        labels = None\n",
    "        if os.path.exists(path2label):\n",
    "            labels = np.loadtxt(path2label).reshape(-1, 5)\n",
    "        else:\n",
    "            print(path2label)\n",
    "\n",
    "        if self.transform:\n",
    "            img, labels = self.transform(img, labels, self.trans_params)\n",
    "\n",
    "        # convert to cells\n",
    "        label_matrix = torch.zeros((self.S1, self.S2, self.C + 5 * self.B)) # here we could remove * self.B\n",
    "        for box in labels:\n",
    "            class_label, x, y, w, h = box\n",
    "            class_label = int(class_label)\n",
    "            # i,j represents the cell row and cell column\n",
    "            i, j = int(self.S1 * y), int(self.S2 * x)\n",
    "            x_cell, y_cell = self.S2 * x - j, self.S1 * y - i\n",
    "\n",
    "            \"\"\"\n",
    "            Calculating the width and height of cell of bounding box,\n",
    "            relative to the cell is done by the following, with\n",
    "            width as the example:\n",
    "\n",
    "            width_pixels = (width*self.image_width)\n",
    "            cell_pixels = (self.image_width /  self.S2)\n",
    "\n",
    "            Then to find the width relative to the cell is simply:\n",
    "            width_pixels/cell_pixels, simplification leads to the\n",
    "            formulas below.\n",
    "            \"\"\"\n",
    "            width_cell, height_cell = (\n",
    "                w * self.S2,\n",
    "                h * self.S1,\n",
    "            )\n",
    "\n",
    "            # If no object already found for specific cell i,j\n",
    "            # Note: This means we restrict to ONE object\n",
    "            # per cell!\n",
    "            if label_matrix[i, j, 1] == 0:\n",
    "                # Set that there exists an object\n",
    "                label_matrix[i, j, 1] = 1\n",
    "\n",
    "                # Box coordinates\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 2:6] = box_coordinates\n",
    "\n",
    "                # Set one hot encoding for class_label\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return img, label_matrix\n",
    "        \n",
    "\n",
    "def hflip(image, labels):\n",
    "    image = TF.hflip(image)\n",
    "    labels[:, 1] = 1 - labels[:, 1]\n",
    "\n",
    "    return image, labels\n",
    "    \n",
    " \n",
    "def vflip(image, labels):\n",
    "    image = TF.vflip(image)\n",
    "    labels[:, 2] = 1 - labels[:, 2]\n",
    "\n",
    "    return image, labels\n",
    "    \n",
    "def grayscale(image):\n",
    "    image = TF.to_grayscale(image, num_output_channels=3)\n",
    "\n",
    "    return image\n",
    "\n",
    "def pad_to_square(img, boxes, pad_value=0, normalized_labels=True):\n",
    "    w, h = img.size\n",
    "    w_factor, h_factor = (w, h) if normalized_labels else (1, 1)\n",
    "\n",
    "    dim_diff = np.abs(h - w)\n",
    "    pad1 = dim_diff // 2\n",
    "    pad2 = dim_diff - pad1\n",
    "\n",
    "    if h <= w:\n",
    "        left, top, right, bottom = 0, pad1, 0, pad2\n",
    "    else:\n",
    "        left, top, right, bottom = pad1, 0, pad2, 0\n",
    "    padding = (left, top, right, bottom)\n",
    "\n",
    "    img_padded = TF.pad(img, padding=padding, fill=pad_value)\n",
    "    w_padded, h_padded = img_padded.size\n",
    "\n",
    "    x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\n",
    "    y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\n",
    "    x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\n",
    "    y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\n",
    "\n",
    "    x1 += padding[0]  # left\n",
    "    y1 += padding[1]  # top\n",
    "    x2 += padding[2]  # right\n",
    "    y2 += padding[3]  # bottom\n",
    "\n",
    "    boxes[:, 1] = ((x1 + x2) / 2) / w_padded\n",
    "    boxes[:, 2] = ((y1 + y2) / 2) / h_padded\n",
    "    boxes[:, 3] *= w_factor / w_padded\n",
    "    boxes[:, 4] *= h_factor / h_padded\n",
    "\n",
    "    return img_padded, boxes\n",
    "\n",
    "def transformer(image, labels, params):\n",
    "    if params[\"pad2square\"] is True:\n",
    "        image, labels = pad_to_square(image, labels)\n",
    "\n",
    "    image = TF.resize(image, params[\"target_size\"])\n",
    "\n",
    "    if random.random() < params[\"p_hflip\"]:\n",
    "        image,labels=hflip(image,labels)\n",
    "        \n",
    "    if random.random() < params[\"p_vflip\"]:\n",
    "        image,labels=vflip(image,labels)\n",
    "        \n",
    "    if params[\"grayscale\"] is True:\n",
    "        image = grayscale(image)\n",
    "\n",
    "    image = TF.to_tensor(image)\n",
    "    targets = torch.zeros((len(labels), 5))\n",
    "    targets = torch.from_numpy(labels)\n",
    "\n",
    "    return image, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilis - Code aus [Aladdin Persson - YOLO/utilils.py](https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/object_detection/YOLO/utils.py) teilweise angepasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
    "    \"\"\"\n",
    "    Calculates intersection over union\n",
    "    Parameters:\n",
    "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
    "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
    "    Returns:\n",
    "        tensor: Intersection over union for all examples\n",
    "    \"\"\"\n",
    "\n",
    "    if box_format == \"midpoint\":\n",
    "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
    "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
    "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
    "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
    "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
    "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
    "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
    "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
    "\n",
    "    if box_format == \"corners\":\n",
    "        box1_x1 = boxes_preds[..., 0:1]\n",
    "        box1_y1 = boxes_preds[..., 1:2]\n",
    "        box1_x2 = boxes_preds[..., 2:3]\n",
    "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
    "        box2_x1 = boxes_labels[..., 0:1]\n",
    "        box2_y1 = boxes_labels[..., 1:2]\n",
    "        box2_x2 = boxes_labels[..., 2:3]\n",
    "        box2_y2 = boxes_labels[..., 3:4]\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    # .clamp(0) is for the case when they do not intersect\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
    "\n",
    "\n",
    "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
    "    \"\"\"\n",
    "    Does Non Max Suppression given bboxes\n",
    "    Parameters:\n",
    "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "    Returns:\n",
    "        list: bboxes after performing NMS given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    assert type(bboxes) == list\n",
    "\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "    bboxes_after_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        chosen_box = bboxes.pop(0)\n",
    "\n",
    "        bboxes = [\n",
    "            box\n",
    "            for box in bboxes\n",
    "            if box[0] != chosen_box[0]\n",
    "               or intersection_over_union(\n",
    "                torch.tensor(chosen_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "                box_format=box_format,\n",
    "            )\n",
    "               < iou_threshold\n",
    "        ]\n",
    "\n",
    "        bboxes_after_nms.append(chosen_box)\n",
    "\n",
    "    return bboxes_after_nms\n",
    "\n",
    "def mean_average_precision(\n",
    "        pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=1, plot=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates mean average precision\n",
    "    Parameters:\n",
    "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
    "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
    "        true_boxes (list): Similar as pred_boxes except all the correct ones\n",
    "        iou_threshold (float): threshold where predicted bboxes is correct\n",
    "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
    "        num_classes (int): number of classes\n",
    "    Returns:\n",
    "        float: mAP value across all classes given a specific IoU threshold\n",
    "    \"\"\"\n",
    "\n",
    "    # list storing all AP for respective classes\n",
    "    average_precisions = []\n",
    "\n",
    "    # used for numerical stability later on\n",
    "    epsilon = 1e-6\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # Go through all predictions and targets,\n",
    "        # and only add the ones that belong to the\n",
    "        # current class c\n",
    "        for detection in pred_boxes:\n",
    "            if detection[1] == c:\n",
    "                detections.append(detection)\n",
    "\n",
    "        for true_box in true_boxes:\n",
    "            if true_box[1] == c:\n",
    "                ground_truths.append(true_box)\n",
    "\n",
    "        # find the amount of bboxes for each training example\n",
    "        # Counter here finds how many ground truth bboxes we get\n",
    "        # for each training example, so let's say img 0 has 3,\n",
    "        # img 1 has 5 then we will obtain a dictionary with:\n",
    "        # amount_bboxes = {0:3, 1:5}\n",
    "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
    "\n",
    "        # We then go through each key, val in this dictionary\n",
    "        # and convert to the following (w.r.t same example):\n",
    "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        # sort by box probabilities which is index 2\n",
    "        detections.sort(key=lambda x: x[2], reverse=True)\n",
    "        TP = torch.zeros((len(detections)))\n",
    "        FP = torch.zeros((len(detections)))\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        # If none exists for this class then we can safely skip\n",
    "        if total_true_bboxes == 0:\n",
    "            continue\n",
    "\n",
    "        for detection_idx, detection in enumerate(detections):\n",
    "            # Only take out the ground_truths that have the same\n",
    "            # training idx as detection\n",
    "            ground_truth_img = [\n",
    "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
    "            ]\n",
    "\n",
    "            num_gts = len(ground_truth_img)\n",
    "            best_iou = 0\n",
    "\n",
    "            for idx, gt in enumerate(ground_truth_img):\n",
    "                iou = intersection_over_union(\n",
    "                    torch.tensor(detection[3:]),\n",
    "                    torch.tensor(gt[3:]),\n",
    "                    box_format=box_format,\n",
    "                )\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt_idx = idx\n",
    "\n",
    "                # we have taken out a single bbox for a particular class in a particular image\n",
    "                # and we have taken out all the ground truth bboxes for that particular image\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                # only detect ground truth detection once\n",
    "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
    "                    # true positive and add this bounding box to seen\n",
    "                    TP[detection_idx] = 1\n",
    "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
    "                else:\n",
    "                    FP[detection_idx] = 1\n",
    "\n",
    "            # if IOU is lower then the detection is a false positive\n",
    "            else:\n",
    "                FP[detection_idx] = 1\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
    "        precisions = torch.div(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "        # torch.trapz for numerical integration\n",
    "        average_precisions.append(torch.trapz(precisions, recalls))\n",
    "\n",
    "        mean_avg_prec = sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        #### Plot fuer Precision Recall Kurve####\n",
    "        #########################################\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.plot(recalls, precisions)\n",
    "            ax.set(xlabel='Recall', ylabel='Precision')\n",
    "            ax.grid(which='both')\n",
    "            print(\"mAP:{}\".format(mean_avg_prec))\n",
    "            plt.show()\n",
    "\n",
    "        return mean_avg_prec\n",
    "\n",
    "\n",
    "def plot_image(image, boxes):\n",
    "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
    "    im = np.array(image)\n",
    "    height, width, _ = im.shape\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(im)\n",
    "\n",
    "    # box[0] is x midpoint, box[2] is width\n",
    "    # box[1] is y midpoint, box[3] is height\n",
    "\n",
    "    # Create a Rectangle potch\n",
    "    for box in boxes:\n",
    "        box = box[2:]\n",
    "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
    "        upper_left_x = box[0] - box[2] / 2\n",
    "        upper_left_y = box[1] - box[3] / 2\n",
    "        rect = patches.Rectangle(\n",
    "            (upper_left_x * width, upper_left_y * height),\n",
    "            box[2] * width,\n",
    "            box[3] * height,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"r\",\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_bboxes(loader, model, iou_threshold, threshold, pred_format=\"cells\", box_format=\"midpoint\", device=\"cuda\", S=(7, 7)):\n",
    "    all_pred_boxes = []\n",
    "    all_true_boxes = []\n",
    "    S1, S2 = S\n",
    "\n",
    "    # make sure model is in eval before get bboxes\n",
    "    model.eval()\n",
    "    train_idx = 0\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = model(x)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        true_bboxes = cellboxes_to_boxes(labels, S=S)\n",
    "        bboxes = cellboxes_to_boxes(predictions, S=S)\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            nms_boxes = non_max_suppression(\n",
    "                bboxes[idx],\n",
    "                iou_threshold=iou_threshold,\n",
    "                threshold=threshold,\n",
    "                box_format=box_format,\n",
    "            )\n",
    "\n",
    "            # if batch_idx == 0 and idx == 0:\n",
    "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
    "            #    print(nms_boxes)\n",
    "\n",
    "            for nms_box in nms_boxes:\n",
    "                all_pred_boxes.append([train_idx] + nms_box)\n",
    "\n",
    "            for box in true_bboxes[idx]:\n",
    "                # many will get converted to 0 pred\n",
    "                if box[1] > threshold:\n",
    "                    all_true_boxes.append([train_idx] + box)\n",
    "\n",
    "            train_idx += 1\n",
    "\n",
    "    model.train()\n",
    "    return all_pred_boxes, all_true_boxes\n",
    "\n",
    "\n",
    "def convert_cellboxes(predictions, S=(7, 7)):\n",
    "    \"\"\"\n",
    "    Converts bounding boxes output from Yolo with\n",
    "    an image split size of S into entire image ratios\n",
    "    rather than relative to cell ratios. Tried to do this\n",
    "    vectorized, but this resulted in quite difficult to read\n",
    "    code... Use as a black box? Or implement a more intuitive,\n",
    "    using 2 for loops iterating range(S) and convert them one\n",
    "    by one, resulting in a slower but more readable implementation.\n",
    "    \"\"\"\n",
    "    S1, S2 = S\n",
    "    predictions = predictions.to(\"cpu\")\n",
    "    batch_size = predictions.shape[0]\n",
    "    predictions = predictions.reshape(batch_size, S1, S2, 11)\n",
    "    bboxes1 = predictions[..., 2:6]\n",
    "    bboxes2 = predictions[..., 7:11]\n",
    "    scores = torch.cat(\n",
    "        (predictions[..., 1].unsqueeze(0), predictions[..., 6].unsqueeze(0)), dim=0\n",
    "    )\n",
    "    best_box = scores.argmax(0).unsqueeze(-1)\n",
    "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
    "    cell_indices_x = torch.arange(S2).repeat(batch_size, S1, 1).unsqueeze(-1)\n",
    "    cell_indices_y = torch.arange(S1).repeat(batch_size, S2, 1).unsqueeze(-1).permute(0, 2, 1, 3)\n",
    "    x = 1 / S2 * (best_boxes[..., :1] + cell_indices_x)\n",
    "    y = 1 / S1 * (best_boxes[..., 1:2] + cell_indices_y)\n",
    "    w = 1 / S2 * best_boxes[..., 2:3]\n",
    "    h = 1 / S1 * best_boxes[..., 3:4]\n",
    "    converted_bboxes = torch.cat((x, y, w, h), dim=-1)\n",
    "    predicted_class = predictions[..., :1].argmax(-1).unsqueeze(-1)\n",
    "    best_confidence = torch.max(predictions[..., 1], predictions[..., 6]).unsqueeze(\n",
    "        -1\n",
    "    )\n",
    "    converted_preds = torch.cat(\n",
    "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
    "    )\n",
    "\n",
    "    return converted_preds\n",
    "\n",
    "\n",
    "def cellboxes_to_boxes(out, S=(7, 7)):\n",
    "    S1, S2 = S\n",
    "    converted_pred = convert_cellboxes(out, S=S).reshape(out.shape[0], S1 * S2, -1)\n",
    "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
    "    all_bboxes = []\n",
    "\n",
    "    for ex_idx in range(out.shape[0]):\n",
    "        bboxes = []\n",
    "\n",
    "        for bbox_idx in range(S1 * S2):\n",
    "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
    "        all_bboxes.append(bboxes)\n",
    "\n",
    "    return all_bboxes\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    \n",
    "    \n",
    "def csv_writer(filename, input_size, hidden_size_FNN, learning_rate, train_loss, val_loss, AP, epoch_counter):\n",
    "    line = [input_size, hidden_size_FNN, learning_rate, train_loss, val_loss, AP, epoch_counter]\n",
    "    with open(filename, mode='a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weitere Hilfsdateien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_writer(filename, input_size, hidden_size_FNN, learning_rate, train_loss, val_loss, AP, epoch_counter):\n",
    "    line = [input_size, hidden_size_FNN, learning_rate, train_loss, val_loss, AP, epoch_counter]\n",
    "    with open(filename, mode='a') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(line)\n",
    "        \n",
    "def txt_writer(filename, input_size, hidden_size_FNN, learning_rate, train_loss_list, val_loss_list):\n",
    "    with open(filename, mode='a') as file:\n",
    "        file.write('\\n'+ \"input_size: \" + str(input_size) + \" hidden_size: \" + str(hidden_size_FNN) + \" learning_rate: \"\n",
    "                      + str(learning_rate) + '\\n')\n",
    "        for i in range(len(train_loss_list)):\n",
    "            file.write(str(i) + \",\" + str(train_loss_list[i]) + \",\" + str(val_loss_list[i]) + '\\n')\n",
    "        \n",
    "def architecture_fn(params):\n",
    "    \"\"\"\n",
    "    Information about architecture config:\n",
    "    Tuple is structured by (kernel_size, filters, stride, padding) \n",
    "    \"M\" is simply maxpooling with stride 2x2 and kernel 2x2\n",
    "    List is structured by tuples and lastly int with number of repeats\n",
    "    \"\"\"\n",
    "    architecture = [(7, 64, 2, 3), \"M\", (3, 192, 1, 1)]\n",
    "    if params[\"input_size\"] != 48:\n",
    "        architecture.append(\"M\")\n",
    "    architecture.append((3, 512, 1, 1))\n",
    "    if params[\"input_size\"] == 192:\n",
    "        architecture.append(\"M\")\n",
    "    architecture.append([(1, 256, 1, 0), (3, 512, 1, 1), 3])\n",
    "    architecture.append((1, 256, 1, 0))\n",
    "            \n",
    "    return architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Engine - Codestrukturierung aus dem aus dem Youtube Video von Abhishek Thakur [End-to-End: Automated Hyperparameter Tuning For Deep Neural Networks](https://www.youtube.com/watch?v=4MK_OJJ82YI&ab_channel=AbhishekThakur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Engine:\n",
    "    def __init__(self, model, optimizer, loss_fn, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, data_loader):\n",
    "        self.model.train()\n",
    "        mean_loss = []\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            score = self.model(x)\n",
    "            loss = self.loss_fn(score, y)\n",
    "            mean_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return sum(mean_loss) / len(mean_loss)\n",
    "\n",
    "    def evaluation(self, data_loader):\n",
    "        self.model.eval()\n",
    "        mean_loss = []\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            score = self.model(x)\n",
    "            loss = self.loss_fn(score, y)\n",
    "            mean_loss.append(loss.item())\n",
    "\n",
    "        return sum(mean_loss) / len(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter anpassen\n",
    " <font color='red'>Ab diesen Abschnitt werden einzelne Codes teilweise wiederholt im Jupyter Notebook wiedergegeben.</font> Für die entsprechenden Hyperparametervariationen wurden folgende Funktionen angepasst: \n",
    "- **objective()**\n",
    "- **run_training()**\n",
    "- **main()** *Code zum Ausführen von run_training()*\n",
    "\n",
    "**Die jeweiligen Überschriften in den Markdown Zellen orientieren sich and den Kapitel Überschriften in den Bericht**\n",
    "Der Code wurde teilweise aus dem aus dem Youtube Video von Abhishek Thakur [End-to-End: Automated Hyperparameter Tuning For Deep Neural Networks](https://www.youtube.com/watch?v=4MK_OJJ82YI&ab_channel=AbhishekThakur) entnommen und entsprechend angepasst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Architekur (siehe Überschrift Bericht)\n",
    "First Gridsearch: learning_rate, input_size, hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective function for Gridsearch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [96, 192]),\n",
    "        \"hidden_size_FFN\": trial.suggest_categorical(\"hidden_size_FFN\", [256, 496]),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.00001, 0.0001, 0.001, 0.01])\n",
    "    }\n",
    "\n",
    "    return run_training(params, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function wrapper to run the Hyperparameteroptimization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_training(params, save_model=False, plot=False):\n",
    "    \n",
    "    target_size_dic = {48: (48, 192), 96: (96, 384), 192: (192, 768)}\n",
    "    trans_params_train={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : 0.0,\n",
    "        \"p_vflip\" : 0.0,\n",
    "    }\n",
    "    \n",
    "    train_dataset = BeeDataset(\"HelperFiles/PathImages_train.txt\", S=SPLIT_SIZE,\n",
    "                               transform=transformer,trans_params=trans_params_train)\n",
    "    test_dataset = BeeDataset(\"HelperFiles/PathImages_test.txt\", S=SPLIT_SIZE,\n",
    "                              transform=transformer, trans_params=trans_params_train)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    \n",
    "    architecture = architecture_fn(params=params)    \n",
    "    \n",
    "    model = Yolov1(hidden_size_FFN=params[\"hidden_size_FFN\"], architecture_config=architecture, \n",
    "                   split_size=SPLIT_SIZE, num_boxes=2, num_classes=1)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=WEIGHT_DECAY)\n",
    "    loss_fn = YoloLoss(S=SPLIT_SIZE)\n",
    "\n",
    "    eng = Engine(model, optimizer, loss_fn, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    early_stopping_iter = 50\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    train_loss_list = list()\n",
    "    val_loss_list = list()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(data_loader=train_loader)\n",
    "        val_loss = eng.evaluation(data_loader=test_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            break\n",
    "            \n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)               \n",
    "\n",
    "    if plot:\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            for idx in range(5):\n",
    "                bboxes = cellboxes_to_boxes(model(x), S=SPLIT_SIZE)\n",
    "                bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "                plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "            break\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "        print(\"Train mAP:{}\".format(mean_avg_prec))\n",
    "    else:\n",
    "        #pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        #mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")      \n",
    "        #csv_writer(filename=\"tmp_export.csv\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   #learning_rate=params[\"learning_rate\"], train_loss=best_train_loss, val_loss=best_loss,\n",
    "                   #AP=mean_avg_prec.item(), epoch_counter=epoch)\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter()\n",
    "def main():\n",
    "    # Grid Search\n",
    "    search_space = {\n",
    "        \"input_size\": [96, 192],\n",
    "        \"hidden_size_FFN\": [256, 496],\n",
    "        \"learning_rate\": [0.00001, 0.0001, 0.001, 0.01]\n",
    "        }\n",
    "    study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=2*2*4)\n",
    "    df = study.trials_dataframe().drop(['state', 'datetime_start', 'datetime_complete'], axis=1)\n",
    "    df.to_csv(r'export_dataframe.csv', index=False, header=True)\n",
    "\n",
    "    print(\"best trial:\")\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.value)\n",
    "    print(trial_.params)\n",
    "\n",
    "    score = run_training(trial_.params, save_model=True, plot=True)\n",
    "    print(score)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Runtime: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Data Augmentation (siehe Überschrift Bericht)\n",
    "Perform Evaluation to asses the influence of Dataaugmentation: no_aug, h_flip, v_flip, hv_flip, gray, hv_flip_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Objective function for Gridsearch\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [192]),\n",
    "        \"hidden_size_FFN\": trial.suggest_categorical(\"hidden_size_FFN\", [496]),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.0001]),\n",
    "        \"data_augmentation\": trial.suggest_categorical(\"data_augmentation\", [\"no_aug\", \"gray\", \"h_flip\", \"v_flip\",\n",
    "                                                                             \"hv_flip\", \"hv_flip_gray\"])\n",
    "    }\n",
    "\n",
    "    return run_training(params, save_model=True, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(params, save_model=False, plot=False):\n",
    "    \n",
    "    target_size_dic = {48: (48, 192), 96: (96, 384), 192: (192, 768)}\n",
    "    aug_hflip, aug_vflip, aug_grayscale = get_augData(params=params)\n",
    "    \n",
    "    trans_params_train={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : aug_hflip,\n",
    "        \"p_vflip\" : aug_vflip,\n",
    "        \"grayscale\": False,\n",
    "    }\n",
    "    \n",
    "    trans_params_test={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : 0.0,\n",
    "        \"p_vflip\" : 0.0,\n",
    "        \"grayscale\" : False,\n",
    "    }\n",
    "    \n",
    "    train_dataset = BeeDataset(\"HelperFiles/PathImages_train.txt\", S=SPLIT_SIZE,\n",
    "                               transform=transformer,trans_params=trans_params_train)\n",
    "    test_dataset = BeeDataset(\"HelperFiles/PathImages_test.txt\", S=SPLIT_SIZE,\n",
    "                              transform=transformer, trans_params=trans_params_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    \n",
    "    architecture = architecture_fn(params=params)    \n",
    "    \n",
    "    model = Yolov1(hidden_size_FFN=params[\"hidden_size_FFN\"], architecture_config=architecture, \n",
    "                   split_size=SPLIT_SIZE, num_boxes=2, num_classes=1)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=WEIGHT_DECAY)\n",
    "    loss_fn = YoloLoss(S=SPLIT_SIZE)\n",
    "\n",
    "    eng = Engine(model, optimizer, loss_fn, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    early_stopping_iter = 100\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    train_loss_list = list()\n",
    "    val_loss_list = list()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(data_loader=train_loader)\n",
    "        val_loss = eng.evaluation(data_loader=test_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            break\n",
    "            \n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        Filename = str(params[\"data_augmentation\"]) + \".pth.tar\"\n",
    "        save_checkpoint(checkpoint, filename=Filename)             \n",
    "\n",
    "    if plot:\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            for idx in range(5):\n",
    "                bboxes = cellboxes_to_boxes(model(x), S=SPLIT_SIZE)\n",
    "                bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "                plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "            break\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "        print(\"AP:{}\".format(mean_avg_prec))\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    else:\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")      \n",
    "        csv_writer(filename=\"tmp_export.csv\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], train_loss=best_train_loss, val_loss=best_loss,\n",
    "                   AP=mean_avg_prec.item(), epoch_counter=epoch)\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tic = time.perf_counter()\n",
    " \n",
    "    # Grid Search\n",
    "    search_space = {\n",
    "        \"input_size\": [192],\n",
    "        \"hidden_size_FFN\": [496],\n",
    "        \"learning_rate\": [0.0001],\n",
    "        \"data_augmentation\": [\"no_aug\", \"gray\", \"h_flip\", \"v_flip\", \"hv_flip\", \"hv_flip_gray\"]\n",
    "    }\n",
    "    study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=6)\n",
    "    df = study.trials_dataframe().drop(['state', 'datetime_start', 'datetime_complete'], axis=1)\n",
    "    df.to_csv(r'export_dataframe.csv', index=False, header=True)\n",
    "\n",
    "    print(\"best trial:\")\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.value)\n",
    "    print(trial_.params)\n",
    "\n",
    "    #score = run_training(trial_.params, save_model=True, plot=True)\n",
    "    #print(score)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Runtime: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dropout und Weight Decay (siehe Überschrift Bericht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [192]),\n",
    "        \"hidden_size_FFN\": trial.suggest_categorical(\"hidden_size_FFN\", [496]),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.0001]),\n",
    "        \"data_augmentation\": trial.suggest_categorical(\"data_augmentation\", [\"hv_flip\"]),\n",
    "        \"weight_decay\" : trial.suggest_categorical(\"weight_decay\", [0.0, 0.001, 0.01, 0.1]),\n",
    "        \"dropout_rate\" : trial.suggest_categorical(\"dropout_rate\", [0.0, 0.2, 0.5]),\n",
    "    }\n",
    "\n",
    "    return run_training(params, save_model=True, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function wrapper to run the Hyperparameteroptimization\n",
    "\"\"\"\n",
    "\n",
    "def run_training(params, save_model=False, plot=False):\n",
    "    \n",
    "    target_size_dic = {48: (48, 192), 96: (96, 384), 192: (192, 768)}\n",
    "    aug_hflip, aug_vflip, aug_grayscale = get_augData(params=params)\n",
    "    \n",
    "    trans_params_train={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : aug_hflip,\n",
    "        \"p_vflip\" : aug_vflip,\n",
    "        \"grayscale\": False,\n",
    "    }\n",
    "    \n",
    "    trans_params_test={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : 0.0,\n",
    "        \"p_vflip\" : 0.0,\n",
    "        \"grayscale\" : False,\n",
    "    }\n",
    "    \n",
    "    train_dataset = BeeDataset(\"HelperFiles/PathImages_train.txt\", S=SPLIT_SIZE,\n",
    "                               transform=transformer,trans_params=trans_params_train)\n",
    "    test_dataset = BeeDataset(\"HelperFiles/PathImages_test.txt\", S=SPLIT_SIZE,\n",
    "                              transform=transformer, trans_params=trans_params_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    \n",
    "    architecture = architecture_fn(params=params)    \n",
    "    \n",
    "    model = Yolov1(hidden_size_FFN=params[\"hidden_size_FFN\"], architecture_config=architecture, \n",
    "                   dropout_rate=params[\"dropout_rate\"], split_size=SPLIT_SIZE, num_boxes=2, num_classes=1)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "    loss_fn = YoloLoss(S=SPLIT_SIZE)\n",
    "\n",
    "    eng = Engine(model, optimizer, loss_fn, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    early_stopping_iter = 50\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    train_loss_list = list()\n",
    "    val_loss_list = list()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(data_loader=train_loader)\n",
    "        val_loss = eng.evaluation(data_loader=test_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_train_loss = train_loss\n",
    "            early_stopping_counter = 0\n",
    "            if save_model == True and epoch > 40:\n",
    "                checkpoint = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                }\n",
    "                Filename = str(params[\"weight_decay\"]) + \"_\" + str(params[\"dropout_rate\"]) + \".pth.tar\"\n",
    "                save_checkpoint(checkpoint, filename=Filename)\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        if early_stopping_counter > early_stopping_iter:\n",
    "            break\n",
    "                       \n",
    "    if plot:\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            for idx in range(5):\n",
    "                bboxes = cellboxes_to_boxes(model(x), S=SPLIT_SIZE)\n",
    "                bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "                plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "            break\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "        print(\"AP:{}\".format(mean_avg_prec))\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], dropout_rate=params[\"dropout_rate\"], weight_decay=params[\"weight_decay\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    else:\n",
    "        load_checkpoint(torch.load(Filename), model, optimizer)\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")      \n",
    "        csv_writer(filename=\"tmp_export.csv\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], dropout_rate=params[\"dropout_rate\"], weight_decay=params[\"weight_decay\"], train_loss=best_train_loss, val_loss=best_loss,\n",
    "                   AP=mean_avg_prec.item(), epoch_counter=epoch)\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], dropout_rate=params[\"dropout_rate\"], weight_decay=params[\"weight_decay\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tic = time.perf_counter()\n",
    " \n",
    "    # Grid Search\n",
    "    search_space = {\n",
    "        \"input_size\": [192],\n",
    "        \"hidden_size_FFN\": [496],\n",
    "        \"learning_rate\": [0.0001],\n",
    "        \"data_augmentation\": [\"hv_flip\"],\n",
    "        \"weight_decay\" : [0.0, 0.001, 0.01, 0.1],\n",
    "        \"dropout_rate\" : [0.0, 0.2, 0.5],\n",
    "    }\n",
    "    study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    df = study.trials_dataframe().drop(['state', 'datetime_start', 'datetime_complete'], axis=1)\n",
    "    df.to_csv(r'export_dataframe.csv', index=False, header=True)\n",
    "\n",
    "    print(\"best trial:\")\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.value)\n",
    "    print(trial_.params)\n",
    "\n",
    "    #score = run_training(trial_.params, save_model=True, plot=True)\n",
    "    #print(score)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Runtime: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian optimization for weight decay parameter\n",
    "siehe Kapitel Dropout und Weight Decay im Bericht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [192]),\n",
    "        \"hidden_size_FFN\": trial.suggest_categorical(\"hidden_size_FFN\", [496]),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.0001]),\n",
    "        \"data_augmentation\": trial.suggest_categorical(\"data_augmentation\", [\"hv_flip\"]),\n",
    "        \"dropout_rate\" : trial.suggest_categorical(\"dropout_rate\", [0.0]),\n",
    "        \"weight_decay\" : trial.suggest_float(\"weight_decay\", 0.1, 10),\n",
    "    }\n",
    "\n",
    "    return run_training(params, save_model=False, plot=False, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function wrapper to run the Hyperparameteroptimization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_training(params, save_model=False, plot=False, early_stopping=True):\n",
    "    \n",
    "    target_size_dic = {48: (48, 192), 96: (96, 384), 192: (192, 768)}\n",
    "    aug_hflip, aug_vflip, aug_grayscale = get_augData(params=params)\n",
    "    \n",
    "    trans_params_train={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : aug_hflip,\n",
    "        \"p_vflip\" : aug_vflip,\n",
    "        \"grayscale\": False,\n",
    "    }\n",
    "    \n",
    "    trans_params_test={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : 0.0,\n",
    "        \"p_vflip\" : 0.0,\n",
    "        \"grayscale\" : False,\n",
    "    }\n",
    "    \n",
    "    train_dataset = BeeDataset(\"HelperFiles/PathImages_train.txt\", S=SPLIT_SIZE,\n",
    "                               transform=transformer,trans_params=trans_params_train)\n",
    "    test_dataset = BeeDataset(\"HelperFiles/PathImages_test.txt\", S=SPLIT_SIZE,\n",
    "                              transform=transformer, trans_params=trans_params_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    \n",
    "    architecture = architecture_fn(params=params)    \n",
    "    \n",
    "    model = Yolov1(hidden_size_FFN=params[\"hidden_size_FFN\"], architecture_config=architecture, \n",
    "                   dropout_rate=params[\"dropout_rate\"], split_size=SPLIT_SIZE, num_boxes=2, num_classes=1)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "    loss_fn = YoloLoss(S=SPLIT_SIZE)\n",
    "\n",
    "    eng = Engine(model, optimizer, loss_fn, device=DEVICE)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_train_loss = np.inf\n",
    "    early_stopping_iter = 50\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    train_loss_list = list()\n",
    "    val_loss_list = list()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = eng.train(data_loader=train_loader)\n",
    "        val_loss = eng.evaluation(data_loader=test_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        print(f\"Epoch: {epoch}, Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "        if early_stopping == True:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_train_loss = train_loss\n",
    "                early_stopping_counter = 0\n",
    "                if save_model == True and epoch > 40:\n",
    "                    checkpoint = {\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                    }\n",
    "                    Filename = \"weightdecay_\" + str(params[\"weight_decay\"]) + \".pth.tar\"\n",
    "                    save_checkpoint(checkpoint, filename=Filename)\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "            if early_stopping_counter > early_stopping_iter:\n",
    "                break\n",
    "        else:\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_train_loss = train_loss\n",
    "                if save_model == True and epoch > 40:\n",
    "                    checkpoint = {\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                    }\n",
    "                    Filename = \"weightdecay_\" + str(params[\"weight_decay\"]) + \".pth.tar\"\n",
    "                    save_checkpoint(checkpoint, filename=Filename)\n",
    "                       \n",
    "    if plot:\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            for idx in range(5):\n",
    "                bboxes = cellboxes_to_boxes(model(x), S=SPLIT_SIZE)\n",
    "                bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "                plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "            break\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
    "        print(\"AP:{}\".format(mean_avg_prec))\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], dropout_rate=params[\"dropout_rate\"], weight_decay=params[\"weight_decay\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    else:\n",
    "        if save_model is True:\n",
    "            load_checkpoint(torch.load(Filename), model, optimizer)\n",
    "        pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=0.4, S=SPLIT_SIZE)\n",
    "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\")      \n",
    "        csv_writer(filename=\"tmp_export.csv\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], dropout_rate=params[\"dropout_rate\"], weight_decay=params[\"weight_decay\"], train_loss=best_train_loss, val_loss=best_loss,\n",
    "                   AP=mean_avg_prec.item(), epoch_counter=epoch)\n",
    "        txt_writer(filename=\"tmp_export.txt\", input_size=params[\"input_size\"], hidden_size_FNN=params[\"hidden_size_FFN\"], \n",
    "                   learning_rate=params[\"learning_rate\"], data_aug=params[\"data_augmentation\"], dropout_rate=params[\"dropout_rate\"], weight_decay=params[\"weight_decay\"], train_loss_list=train_loss_list, val_loss_list=val_loss_list)\n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    tic = time.perf_counter()\n",
    " \n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    df = study.trials_dataframe().drop(['state', 'datetime_start', 'datetime_complete'], axis=1)\n",
    "    df.to_csv(r'export_dataframe.csv', index=False, header=True)\n",
    "\n",
    "    print(\"best trial:\")\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.value)\n",
    "    print(trial_.params)\n",
    "\n",
    "    score = run_training(trial_.params, save_model=True, plot=False, early_stopping=False)\n",
    "    print(score)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Runtime: {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frames per second (FPS) messen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"input_size\": trial.suggest_categorical(\"input_size\", [192]),\n",
    "        \"hidden_size_FFN\": trial.suggest_categorical(\"hidden_size_FFN\", [496]),\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.0001]),\n",
    "        \"data_augmentation\": trial.suggest_categorical(\"data_augmentation\", [\"no_aug\", \"gray\", \"h_flip\", \"v_flip\",\n",
    "                                                                             \"hv_flip\", \"hv_flip_gray\"]),\n",
    "        \"dropout_rate\" : trial.suggest_categorical(\"dropout_rate\", [0.0]),\n",
    "        \"weight_decay\" : trial.suggest_categorical(\"weight_decay\", [0.2]),\n",
    "    }\n",
    "\n",
    "    return run_training(params, save_model=True, plot=False, early_stopping=True, ap=True)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function wrapper to run the Hyperparameteroptimization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_training(params, save_model=False, plot=False, early_stopping=True, ap=True):\n",
    "            \n",
    "    target_size_dic = {48: (48, 192), 96: (96, 384), 192: (192, 768)}\n",
    "    aug_hflip, aug_vflip, aug_grayscale = get_augData(params=params)\n",
    "    \n",
    "    trans_params_train={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : aug_hflip,\n",
    "        \"p_vflip\" : aug_vflip,\n",
    "        \"grayscale\": False,\n",
    "    }\n",
    "    \n",
    "    trans_params_test={\n",
    "        \"target_size\": target_size_dic[params[\"input_size\"]],\n",
    "        \"pad2square\": False,\n",
    "        \"p_hflip\" : 0.0,\n",
    "        \"p_vflip\" : 0.0,\n",
    "        \"grayscale\" : False,\n",
    "    }\n",
    "    \n",
    "    train_dataset = BeeDataset(\"HelperFiles/PathImages_train.txt\", S=SPLIT_SIZE,\n",
    "                               transform=transformer,trans_params=trans_params_train)\n",
    "    test_dataset = BeeDataset(\"HelperFiles/PathImages_test.txt\", S=SPLIT_SIZE,\n",
    "                              transform=transformer, trans_params=trans_params_test)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                              pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                             pin_memory=PIN_MEMORY, shuffle=True, drop_last=True)\n",
    "    \n",
    "    architecture = architecture_fn(params=params)    \n",
    "    \n",
    "    model = Yolov1(hidden_size_FFN=params[\"hidden_size_FFN\"], architecture_config=architecture, \n",
    "                   dropout_rate=params[\"dropout_rate\"], split_size=SPLIT_SIZE, num_boxes=2, num_classes=1).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n",
    "    \n",
    "    if ap == True:\n",
    "        LOAD_MODEL_FILE = \"weightdecay_0_2.pth.tar\"\n",
    "        LOAD_MODEL = True\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
    "            \n",
    "        \n",
    "        iou_threshold = [0.5]\n",
    "        #iou_threshold = np.arange(0.05,1,0.05).tolist()\n",
    "        mean_avg_prec_list = list()\n",
    "        for i, iou in enumerate(iou_threshold):\n",
    "            break\n",
    "            pred_boxes, target_boxes = get_bboxes(test_loader, model, iou_threshold=0.5, threshold=threshold[i], S=SPLIT_SIZE)\n",
    "            mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=iou, box_format=\"midpoint\", plot=False, txt=True)\n",
    "            print(\"iou: \" + str(mean_avg_prec) + \", threshold: \")\n",
    "        ################################################################################################\n",
    "        ##### Important Part ###########################################################################\n",
    "        ################################################################################################\n",
    "        import time\n",
    "        batch_size = 16\n",
    "        counter = 0\n",
    "        tic = time.perf_counter()\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            counter += 1\n",
    "            for idx in range(len(x)):\n",
    "                bboxes = cellboxes_to_boxes(model(x), S=SPLIT_SIZE)\n",
    "                bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "                #plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Time {toc - tic:0.4f} seconds\")\n",
    "        print(counter)\n",
    "        print(f\"FPS: {counter*batch_size/(toc - tic)}\")\n",
    "        import sys\n",
    "        sys.exit()\n",
    "        ###############################################################################################\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    tic = time.perf_counter()\n",
    " \n",
    "    # Grid Search\n",
    "    search_space = {\n",
    "        \"input_size\": [192],\n",
    "        \"hidden_size_FFN\": [496],\n",
    "        \"learning_rate\": [0.0001],\n",
    "        \"data_augmentation\": [\"hv_flip\"],\n",
    "        \"dropout_rate\" : [0.0],\n",
    "        \"weight_decay\" : [0.2],\n",
    "    }\n",
    "    study = optuna.create_study(sampler=optuna.samplers.GridSampler(search_space), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    df = study.trials_dataframe().drop(['state', 'datetime_start', 'datetime_complete'], axis=1)\n",
    "    df.to_csv(r'export_dataframe.csv', index=False, header=True)\n",
    "\n",
    "    print(\"best trial:\")\n",
    "    trial_ = study.best_trial\n",
    "    print(trial_.value)\n",
    "    print(trial_.params)\n",
    "\n",
    "    #score = run_training(trial_.params, save_model=True, plot=False, early_stopping=False)\n",
    "    #print(score)\n",
    "\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Runtime: {toc - tic:0.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
