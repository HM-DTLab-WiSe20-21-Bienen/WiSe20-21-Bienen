{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 2 Implementation of MobileNet2 and Yolo2 from scratch\n",
    "# Sources and Github repos used\n",
    "https://github.com/jmpap/YOLOV2-Tensorflow-2.0/blob/master/Yolo_V2_tf_2.ipynb \\\n",
    "https://github.com/zzh8829/yolov3-tf2 \\\n",
    "https://github.com/zzxvictor/YOLO_Explained \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define MobilNet Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Necessary basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, DepthwiseConv2D, AveragePooling2D\n",
    "from tensorflow.keras.activations import relu, softmax\n",
    "\n",
    "\n",
    "class BConv2D(tf.keras.Model):\n",
    "\n",
    "    \"\"\"Convolution Block with Batchnorm and relu_6\"\"\"\n",
    "\n",
    "    def __init__(self, *args, conv='normal', alpha=0., max_value=6, **kwargs):\n",
    "        super().__init__()\n",
    "        if 'normal' == conv:\n",
    "            self.conv = Conv2D(*args, **kwargs)\n",
    "        elif 'depth' == conv:\n",
    "            self.conv = DepthwiseConv2D(*args, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"conv parameter must be 'normal' or 'depth'\")\n",
    "        self.batch = BatchNormalization()\n",
    "        self.alpha = alpha\n",
    "        self.max_value = max_value\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batch(x)\n",
    "        return relu(x, alpha=self.alpha, max_value=self.max_value)\n",
    "\n",
    "\n",
    "class BottleneckBlock(tf.keras.Model):\n",
    "    \"\"\"Convolution Block with expansion, conv and projection layer, see mobileNetV2\n",
    "        ex = expansion factor\"\"\"\n",
    "\n",
    "    def __init__(self, channels_in, channels_out, kernel=3, ex=6, stride =1, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.exp = BConv2D(channels_in * ex, 1, **kwargs)\n",
    "        self.conv = BConv2D(kernel, conv='depth', strides=stride, **kwargs)\n",
    "        self.proj = Conv2D(channels_out, 1, **kwargs)\n",
    "        self.batch = BatchNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.exp(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.proj(x)\n",
    "        x = self.batch(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BottleneckSequence(tf.keras.Model):\n",
    "    \"\"\"Sequence of bottleneck-blocks with length n. First layer of sequence is performed\n",
    "    with stride x if given, remaining are stride 1 with residual connections added respectively\"\"\"\n",
    "\n",
    "    def __init__(self, channels_in, channels_out, depth, kernel=3, ex=6, stride=2, **kwargs):\n",
    "        super().__init__()\n",
    "        if depth == 1:\n",
    "            raise ValueError(\"Minimum Depth must be two. For depth=1, use 'BottleneckBlock' instead\")\n",
    "        # calculate length of middle sequence, start and end layer differs\n",
    "        length = depth-2\n",
    "        self.initial = BottleneckBlock(channels_in, channels_in, kernel, ex, stride, **kwargs)\n",
    "        if length >= 1:\n",
    "            self.middle = [BottleneckBlock(channels_in, channels_in, kernel, ex, **kwargs) for _ in range(depth)]\n",
    "        else:\n",
    "            self.middle = None\n",
    "        self.end = BottleneckBlock(channels_in, channels_out, kernel, ex, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.initial(x)\n",
    "        if self.middle is not None:\n",
    "            for layer in self.middle:\n",
    "                res = tf.identity(x)\n",
    "                x = layer(x)\n",
    "                # will fail if padding != same, assert equal tensor shape\n",
    "                x += res\n",
    "        x = self.end(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileNetV2(tf.keras.Model):\n",
    "\n",
    "    \"\"\"Input: Type: 'Base': returns the last three Output-Blocks in ascending order\n",
    "                            , skips softmax and pooling\n",
    "                    'Full': for Classification of k Classes\n",
    "              k: Number of classes to classify if Type = 'Full' \"\"\"\n",
    "\n",
    "    def __init__(self, expansion_factor=6, net_type='base', k=1): \n",
    "        super().__init__()\n",
    "\n",
    "        kwargs = {'padding': 'same'}\n",
    "        self.type = net_type.lower()\n",
    "\n",
    "        # feature extraction layers\n",
    "        self.layer1 = BConv2D(32, 3, strides=2, **kwargs)\n",
    "        self.bottle2 = BottleneckBlock(16, 16, ex=1, **kwargs)\n",
    "        self.sequ3 = BottleneckSequence(16, 24, 2, ex=expansion_factor, **kwargs)\n",
    "        self.sequ4 = BottleneckSequence(24, 32, 3, ex=expansion_factor, **kwargs)\n",
    "        self.sequ5 = BottleneckSequence(32, 64, 4, ex=expansion_factor, **kwargs)\n",
    "        self.sequ6 = BottleneckSequence(64, 96, 3, ex=expansion_factor, stride=1, **kwargs)\n",
    "        self.sequ7 = BottleneckSequence(96, 160, 3, ex=expansion_factor, **kwargs)\n",
    "        self.bottle8 = BottleneckBlock(160, 320, ex=expansion_factor, **kwargs)\n",
    "        self.layer9 = BConv2D(1280, 1, **kwargs)\n",
    "\n",
    "        if self.type == 'full':\n",
    "            # detector layers, change them for yolo or ssd detectors\n",
    "            # as of know, mobilenet is solely able to classify, not detect!\n",
    "            self.avg10 = AveragePooling2D(pool_size=(7, 7))\n",
    "            self.out = Conv2D(k, 1, **kwargs)  # k classes\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bottle2(x)\n",
    "        x = self.sequ3(x)\n",
    "        out_1 = self.sequ4(x)\n",
    "        x = self.sequ5(out_1)\n",
    "        out_2 = self.sequ6(x)\n",
    "        x = self.sequ7(out_2)\n",
    "        x = self.bottle8(x)\n",
    "        out_3 = self.layer9(x)\n",
    "        if self.type == 'full':\n",
    "            out_3 = self.avg10(out_3)\n",
    "            out_3 = self.out(out_3)\n",
    "            return softmax(out_3)\n",
    "        else:\n",
    "            return out_3, out_2, out_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Yolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, UpSampling2D\n",
    "from tensorflow.keras.losses import binary_crossentropy, sparse_categorical_crossentropy\n",
    "from tensorflow.keras.activations import sigmoid, softmax\n",
    "\n",
    "\n",
    "class YoloDetectBlock(tf.keras.Model):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        kwargs = {'padding': 'same'}\n",
    "\n",
    "        self.l1 = BConv2D(base, 1, alpha=0.1, max_value=None, **kwargs)\n",
    "        self.l2 = BConv2D(base*2, 3, alpha=0.1, max_value=None, **kwargs)\n",
    "        self.l3 = BConv2D(base, 1, alpha=0.1, max_value=None, **kwargs)\n",
    "        self.l4 = BConv2D(base*2, 3, alpha=0.1, max_value=None, **kwargs)\n",
    "        self.l5 = BConv2D(base, 1, alpha=0.1, max_value=None, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.l4(x)\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "class YOLOVX(tf.keras.Model):\n",
    "    def __init__(self, n_bb=2, classes=2, iou_thresh = 0.5, coeff_bb=5, coeff_obj=5, coeff_nonObj=0.5):\n",
    "        super(YOLOVX, self).__init__()\n",
    "        # set params\n",
    "        self.n_bb = n_bb\n",
    "        self.classes = classes\n",
    "        self.anchors = [(1, 2), (3, 4)] \n",
    "        self.iou_thresh = iou_thresh\n",
    "        self.coeff_bb = coeff_bb\n",
    "        self.coeff_obj = coeff_obj\n",
    "        self.coeff_nonObj = coeff_nonObj\n",
    "\n",
    "        self.b1_layer1 = YoloDetectBlock(512)\n",
    "        self.b1_layer2 = BConv2D(512, 3, alpha=0.1, max_value=None, padding='same')\n",
    "\n",
    "        self.b2_layer1 = YoloDetectBlock(256)\n",
    "        self.b2_layer2 = BConv2D(256, 3, alpha=0.1, max_value=None, strides =2, padding='same')\n",
    "\n",
    "        self.b3_layer1 = YoloDetectBlock(128)\n",
    "        self.b3_layer2 = BConv2D(128, 3, alpha=0.1, max_value=None, strides = 4, padding='same')\n",
    "\n",
    "        self.pred = Conv2D(n_bb * (5+classes), 1, padding='same')\n",
    "\n",
    "    def call(self, input1, input2, input3):\n",
    "        x3 = self.b3_layer1(input3)\n",
    "        x3 = self.b3_layer2(x3)\n",
    "\n",
    "        x2 = self.b2_layer1(input2)\n",
    "        x2 = self.b2_layer2(x2)\n",
    "\n",
    "        x1 = self.b1_layer1(input1)\n",
    "        x1 = self.b1_layer2(x1)\n",
    "\n",
    "        x = tf.concat([x1,x2,x3], axis=-1)\n",
    "        pred = self.pred(x)\n",
    "\n",
    "        b, h, w, *_ = pred.shape\n",
    "        pred = tf.reshape(pred, [b, h, w, self.n_bb, 4 + 1 + self.classes])\n",
    "        return pred\n",
    "    \n",
    "    def set_anchors(self, anchors):\n",
    "        self.anchors = anchors\n",
    "\n",
    "    def __get_grid__(self, shape):\n",
    "        hIndex = tf.reshape(tf.range(start=0, limit=shape[0]), (shape[0], 1))\n",
    "        hIndex = tf.tile(hIndex, [1, shape[1]])  # expand in the height direction\n",
    "        wIndex = tf.reshape(tf.range(start=0, limit=shape[1]), (1, shape[1]))\n",
    "        wIndex = tf.tile(wIndex, [shape[0], 1])  # expand in the width direction\n",
    "        grid = tf.stack([wIndex, hIndex], axis=-1)\n",
    "        grid = tf.reshape(grid, shape=(1, *shape, 1, 2))  # reshape the offset so that it can add to boxXY directly\n",
    "        return tf.cast(grid, dtype=tf.float32)\n",
    "\n",
    "    def __getIOU__(self, box1, box2):\n",
    "        mini = tf.math.maximum(box1[..., 0:2] - box1[..., 2:4] / 2, box2[..., 0:2] - box2[..., 2:4] / 2)\n",
    "        maxi = tf.math.minimum(box1[..., 0:2] + box1[..., 2:4] / 2, box2[..., 0:2] + box2[..., 2:4] / 2)\n",
    "        interWH = tf.math.maximum(maxi - mini, 0)\n",
    "        interArea = interWH[..., 0] * interWH[..., 1]\n",
    "        area1 = box1[..., 2] * box1[..., 3]\n",
    "        area2 = box2[..., 2] * box2[..., 3]\n",
    "        return interArea / (area1 + area2 - interArea)\n",
    "\n",
    "    def process_output(self, output):\n",
    "        # output is already reshaped to (batch, grid, grid, nboxes, (4 + 1 + classes)\n",
    "        b, h, w, *_ = output.shape\n",
    "        box_xy = sigmoid(output[..., :2])\n",
    "        grid = self.__get_grid__((h, w))\n",
    "        grid = tf.cast(grid, tf.float32)\n",
    "        anchors = tf.cast(tf.reshape(self.anchors, (1, 1, 1, self.n_bb, 2)), tf.float32)\n",
    "        box_xy = grid + box_xy\n",
    "\n",
    "        box_wh = tf.math.exp(output[..., 2:4])\n",
    "        box_wh = box_wh * anchors\n",
    "\n",
    "        obj_score = sigmoid(output[..., 4:5])\n",
    "        class_pred = softmax(output[..., 5:])\n",
    "\n",
    "        return box_xy, box_wh, obj_score, class_pred\n",
    "\n",
    "    def getBB(self, box_xy, box_wh, scale=(32, 32)):\n",
    "        # rescale bounding boxes to original image\n",
    "        x1y1 = box_xy - box_wh / 2  # top left\n",
    "        x2y2 = box_xy + box_wh / 2  # bottom right\n",
    "        bb = tf.concat([x1y1, x2y2], axis=-1)\n",
    "        shape = tf.stack([*scale, *scale])\n",
    "        shape = tf.reshape(shape, [1, 4])\n",
    "        shape = tf.cast(shape, tf.float32)\n",
    "        return bb * shape\n",
    "\n",
    "    def filterBB(self, bb, obj_score, class_pred, maxBox=20, score_thresh=0.5, iou_thresh=0.5): #TODO: als Parameter setzen\n",
    "\n",
    "        boxScore = obj_score * class_pred\n",
    "        boxClass = tf.argmax(boxScore, axis=-1)\n",
    "        boxScore = tf.math.reduce_max(boxScore, axis=-1)\n",
    "        mask = boxScore >= score_thresh\n",
    "\n",
    "        # filter out low-confidence boxes\n",
    "        boxes = tf.boolean_mask(bb, mask)  # -> array[0...N]\n",
    "        scores = tf.boolean_mask(boxScore, mask)\n",
    "        classes = tf.boolean_mask(boxClass, mask)\n",
    "\n",
    "        # perform nms\n",
    "        idx = tf.image.non_max_suppression(boxes, scores, maxBox, iou_threshold=iou_thresh)\n",
    "        boxes = tf.gather(boxes, idx)\n",
    "        scores = tf.gather(scores, idx)\n",
    "        classes = tf.gather(classes, idx)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "\n",
    "    def raw2Box(self, output, maxBox=20, scoreThresh=0.5, iouThresh=0.5):\n",
    "\n",
    "        # convert raw yolo output\n",
    "        box_xy, box_wh, obj_scores, class_probs = self.process_output(output)\n",
    "        # scale gridscale back to imagescale, return bb in shape (x1,y1,x2,y2)\n",
    "        boxes = self.getBB(box_xy, box_wh)\n",
    "\n",
    "        for bb, score, prob in zip(boxes, obj_scores, class_probs):\n",
    "            # filter out low confidence boxes and do nms\n",
    "            f_bb, f_score, f_prob = self.filterBB(bb, score, prob, maxBox=maxBox, score_thresh=scoreThresh, iou_thresh=iouThresh)\n",
    "            yield f_bb, f_score, f_prob\n",
    "\n",
    "    def loss(self, gt, output):\n",
    "        b, h, w, *_ = output.shape\n",
    "        #get obj mask\n",
    "        # grid grid, nbb, (xywh, obj, class)\n",
    "        mask = tf.cast(gt[...,4], tf.float32)\n",
    "\n",
    "        # =========BB-Loss===============================\n",
    "        # get transformed output\n",
    "        box_xy, box_wh, obj_score, class_pred = self.process_output(output)\n",
    "        grid = self.__get_grid__((h,w))\n",
    "\n",
    "        # get the right labels (pre-processed ofc)\n",
    "        anchors = tf.cast(tf.reshape(self.anchors, (1, 1, 1, self.n_bb, 2)), tf.float32)\n",
    "        # [grid, grid, nbb, (xywh, obj, classe)]\n",
    "        gt_xy = gt[...,0:2] + grid\n",
    "        gt_wh = gt[...,2:4] * anchors\n",
    "\n",
    "        xy_loss = mask * self.coeff_bb * tf.reduce_sum(tf.square(gt_xy - box_xy), axis=-1)\n",
    "        wh_loss = mask * self.coeff_bb * tf.reduce_sum(tf.square(tf.sqrt(gt_wh) - tf.sqrt(box_wh)), axis=-1)\n",
    "\n",
    "        # reshape mask for further use\n",
    "        mask = tf.expand_dims(mask, axis=-1)\n",
    "        #===========Confidence-Loss=============================\n",
    "        iou = self.__getIOU__(tf.concat([box_xy, box_wh], axis=-1), tf.concat([gt_xy, gt_wh], axis=-1))\n",
    "        best_iou = tf.reduce_max(iou, axis=-1, keepdims=True)\n",
    "        non_obj = tf.cast(tf.expand_dims(best_iou < self.iou_thresh, axis=-1), tf.float32)\n",
    "        non_obj_loss = non_obj * (1.0 - mask) * self.coeff_nonObj * tf.square(0-obj_score)\n",
    "        obj_loss = mask * self.coeff_obj * tf.square(obj_score - tf.expand_dims(iou, axis=-1))\n",
    "\n",
    "        #==========Class-Loss=============================\n",
    "        gt_class = tf.cast(gt[...,5], tf.int32)\n",
    "        gt_class = tf.one_hot(gt_class, depth=self.classes)\n",
    "        class_loss = mask * tf.square(gt_class-class_pred)\n",
    "        \n",
    "\n",
    "        # sum over batches\n",
    "        xy_loss = tf.reduce_sum(xy_loss, axis=[1,2,3])\n",
    "        wh_loss = tf.reduce_sum(wh_loss, axis=[1,2,3])\n",
    "        obj_loss = tf.reduce_sum(non_obj_loss+obj_loss, axis=[1,2,3,4])\n",
    "        class_loss = tf.reduce_sum(class_loss, axis=[1,2,3,4])\n",
    "        return xy_loss,wh_loss, obj_loss, class_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Final Stacked-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, expansion_factor, n_bb, n_classes, iou_thresh = 0.5,  coeff_bb=5, coeff_obj=5, coeff_nonObj=0.5):\n",
    "        super(Detector, self).__init__()\n",
    "        \n",
    "        # Need to use pre-defined model due to different building API's (Subclassed vs Functional) -> cant load weights\n",
    "        #self.backbone = MobileNetV2(expansion_factor=expansion_factor)\n",
    "        self.backbone = tf.keras.applications.MobileNetV2(input_shape=(96, 416, 3), include_top=False, classes=1)\n",
    "        self.detector = YOLOVX(n_bb=n_bb, classes=n_classes, iou_thresh = iou_thresh,\n",
    "                               coeff_bb=coeff_bb, coeff_obj=coeff_obj,\n",
    "                               coeff_nonObj=coeff_nonObj)\n",
    "        self.input3 = tf.keras.models.Model(inputs=self.backbone.input,\n",
    "                                     outputs= self.backbone.get_layer(\"block_6_expand_relu\").output)\n",
    "        self.input2 = tf.keras.models.Model(inputs=self.backbone.input,\n",
    "                                     outputs= self.backbone.get_layer(\"block_13_expand_relu\").output)\n",
    "        \n",
    "        self.backbone.trainable = False\n",
    "        self.input3.trainable = False\n",
    "        self.input2.trainable = False\n",
    "\n",
    "    def call(self, input1):\n",
    "        x1 = self.backbone(input1)\n",
    "        x2 = self.input2(input1)\n",
    "        x3 = self.input3(input1)\n",
    "        return self.detector(x1,x2,x3)\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return self.detector.loss(y_true, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import io\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_HEIGHT = 371.0/96.0\n",
    "SCALE_WIDHT = 1613.0/416.0\n",
    "SCALE = 32\n",
    "GRID_W = 13\n",
    "GRID_H = 3\n",
    "anchors = [1.0, 2.0, 3.0, 4.0]\n",
    "anchors_count = len(anchors) // 2\n",
    "anchors = np.array(anchors)\n",
    "anchors = anchors.reshape(len(anchors) // 2, 2)\n",
    "detector_mask = np.zeros((GRID_H,GRID_W , anchors_count, 1))\n",
    "matching_true_boxes = np.zeros((GRID_H,GRID_W , anchors_count, 6))\n",
    "WIDTH = 1613\n",
    "\n",
    "\n",
    "def scaleToImgSize(height,width,x,y):\n",
    "    return int(height/SCALE_HEIGHT),int(width/SCALE_WIDHT),int(x/SCALE_WIDHT),int(y/SCALE_HEIGHT)\n",
    "\n",
    "def format_annotations(img, annotations):\n",
    "    img = tf.image.resize(img, (96,416)) \n",
    "    img = tf.cast(img, tf.float32)\n",
    "\n",
    "    #Umwandeln der annotations\n",
    "    detector_mask = np.zeros((GRID_H,GRID_W , anchors_count, 1))\n",
    "    matching_true_boxes = np.zeros((GRID_H,GRID_W , anchors_count, 6))       \n",
    "\n",
    "    for p in annotations:\n",
    "        height = p['height']\n",
    "        width  = p['width']\n",
    "        x = p['left']\n",
    "        y = p['top']\n",
    "        height,width,x,y = scaleToImgSize(height,width,x,y)\n",
    "        bbx = (x+(width/2))\n",
    "        bby = (y+(height/2))\n",
    "        index_x = int(bbx//SCALE)\n",
    "        index_y = int(bby//SCALE)\n",
    "        tx = (bbx-(index_x*SCALE))/SCALE\n",
    "        ty = (bby-(index_y*SCALE))/SCALE\n",
    "        w = width / SCALE\n",
    "        h = height / SCALE\n",
    "\n",
    "        batch_true_boxes_grid = np.array([w,h,tx,ty,1.0,0.0])            \n",
    "\n",
    "        if w * h > 0:  # box exists\n",
    "             # calculate iou between box and each anchors and find best anchors\n",
    "            best_iou = 0\n",
    "            best_anchor = 0\n",
    "            for i in range(anchors_count):\n",
    "                # iou (anchor and box are shifted to 0,0)\n",
    "                intersect = np.minimum(w, anchors[i, 0]) * np.minimum(h, anchors[i, 1])\n",
    "                union = (anchors[i, 0] * anchors[i, 1]) + (w * h) - intersect\n",
    "                iou = intersect / union\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_anchor = i\n",
    "            # localize box in detector_mask and matching true_boxes\n",
    "            if best_iou > 0:\n",
    "                x_coord = np.floor(x).astype('int')\n",
    "                y_coord = np.floor(y).astype('int')\n",
    "                detector_mask[index_y, index_x, best_anchor] = 1\n",
    "                #old yolo_box = np.array([tx, ty, w, h, 1.0, 1, 0.0])\n",
    "                #x, y, w, h, obj,bee,other                            \n",
    "                yolo_box = np.array([tx, ty, w, h, 1.0, 0.0])\n",
    "                matching_true_boxes[index_y, index_x, best_anchor] = yolo_box\n",
    "    return img, matching_true_boxes\n",
    "\n",
    "def get_data():\n",
    "    images = np.empty((4000, 96, 416, 3), dtype=np.float32)\n",
    "    labels = np.empty((4000, 3, 13, anchors_count, 6), dtype = np.float32)\n",
    "    #with smart_open('s3://labeling-test-ai-supported-enbeemoresearchdata-k8wchfbly0wb/bee-labeling-2k-batch-01/manifests/output/output.manifest', 'rb') as s3_source:\n",
    "        #for count, line in enumerate(s3_source):\n",
    "    with open(\"annotations/annotations.json\") as f:\n",
    "        count = 0\n",
    "        my_list = f.readlines()\n",
    "        mydict = {}\n",
    "        myVolumeDict = {}       \n",
    "        for x in my_list:\n",
    "            if (x != \"\\n\"):\n",
    "                lineAsJson = json.loads(x)\n",
    "                source = lineAsJson['source-ref']           \n",
    "                splitted_source=source.split('/')        \n",
    "                annotations = lineAsJson[\"bee-labeling-2k-batch-01\"][\"annotations\"]\n",
    "                s3_bucket, s3_key = splitted_source[2], splitted_source[3]\n",
    "\n",
    "                # Load image from s3 (old)\n",
    "                #with io.BytesIO() as f:\n",
    "                # boto3.client(\"s3\").download_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)\n",
    "                #f.seek(0)\n",
    "\n",
    "                # Load image from local directory (faster)\n",
    "                path = \"images/\" + s3_key\n",
    "                originalImage = plt.imread(path, format='png')\n",
    "\n",
    "                img, matching_true_boxes = format_annotations(originalImage, annotations)\n",
    "                images[count, :,:,:] = img\n",
    "                labels[count, :,:,:,:] = matching_true_boxes\n",
    "                #Flipped Image\n",
    "                flippedImage = cv2.flip(originalImage, 1)\n",
    "                flippedAnnotations = annotations\n",
    "                for annotation in flippedAnnotations:\n",
    "                    annotation['left'] = WIDTH - annotation['left'] - annotation['width']\n",
    "                imgFlipped, matching_true_boxesFlipped = format_annotations(flippedImage, flippedAnnotations)\n",
    "                \n",
    "                images[count + 2000, :,:,:] = imgFlipped\n",
    "                labels[count + 2000, :,:,:,:] = matching_true_boxesFlipped\n",
    "\n",
    "                if count % 100 == 0:\n",
    "                    print(\"read files: {}\".format(count))\n",
    "                count += 1         \n",
    "\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the data (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_images, data_labels = get_data()\n",
    "#data = tf.data.Dataset.from_tensor_slices((data_images, data_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_img, matching_true_boxes in data:\n",
    "    one_img = np.array(one_img)\n",
    "   \n",
    "    SCALE = 32\n",
    "    for i in range(3):\n",
    "        for j in range(13):\n",
    "            if matching_true_boxes[i][j][0][4] > 0.0:\n",
    "                center_x = j*SCALE+matching_true_boxes[i][j][0][0]*SCALE\n",
    "                center_y = i*SCALE+matching_true_boxes[i][j][0][1]*SCALE\n",
    "\n",
    "                upper_left = (int(center_x-matching_true_boxes[i][j][0][2]*SCALE),int( center_y-matching_true_boxes[i][j][0][3]*SCALE))\n",
    "                under_right = (int(center_x+matching_true_boxes[i][j][0][2]*SCALE),int(center_y+matching_true_boxes[i][j][0][3]*SCALE))\n",
    "                one_img = cv2.rectangle(one_img, upper_left, under_right, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "            elif matching_true_boxes[i][j][1][4] > 0.0:\n",
    "\n",
    "                center_x = j*SCALE+matching_true_boxes[i][j][1][0]*SCALE\n",
    "                center_y = i*SCALE+matching_true_boxes[i][j][1][1]*SCALE\n",
    "\n",
    "                upper_left = (int(center_x-matching_true_boxes[i][j][1][2]*SCALE),int(center_y-matching_true_boxes[i][j][1][3]*SCALE))\n",
    "                under_right = (int(center_x+matching_true_boxes[i][j][1][2]*SCALE),int(center_y+matching_true_boxes[i][j][1][3]*SCALE))\n",
    "                one_img = cv2.rectangle(one_img, upper_left, under_right, (0, 0, 255), 2)\n",
    "               \n",
    "                \n",
    "    plt.figure(figsize=(2,2))\n",
    "    f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize=(10, 10))\n",
    "    ax1.imshow(one_img*255)\n",
    "    ax1.set_title('image')\n",
    "\n",
    "    ax2.matshow(matching_true_boxes[:,:,0,4]) # YOLO Confidence value\n",
    "    ax2.set_title('mask1')\n",
    "    ax2.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    ax3.matshow(matching_true_boxes[:,:,1,4]) # YOLO Confidence value\n",
    "    ax3.set_title('mask2')\n",
    "    ax3.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax4.matshow(matching_true_boxes[:,:,2,4]) # YOLO Confidence value\n",
    "    ax4.set_title('mask3')\n",
    "    ax4.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    ax5.matshow(tf.reduce_sum(matching_true_boxes[:,:,:,4], axis=-1)) # YOLO Confidence value\n",
    "    ax5.set_title('summed mask')\n",
    "    ax5.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def cast_img(img):\n",
    "    return tf.cast(img*255, dtype=tf.uint8)\n",
    "\n",
    "def get_session_name():\n",
    "    current_time = int(time.time())\n",
    "    description = input(\"Describe the session: \")\n",
    "    session_name = \"s{}: {}\".format(current_time, description)\n",
    "    print(session_name)\n",
    "    return session_name\n",
    "\n",
    "def load_fake_dataset():\n",
    "    img = tf.image.decode_jpeg(\n",
    "        open(\"/content/drive/My Drive/tests/ich.jpg\", 'rb').read(), channels=3)\n",
    "    img = tf.image.resize(img, (96,416))\n",
    "    # shape grid, grid, nbb, (xywh, obj, classes)\n",
    "    labels = tf.zeros([3, 13, 3, 4+1+2], tf.float32)\n",
    "    data = tf.data.Dataset.from_tensor_slices(([img], [labels]))\n",
    "    data = data.batch(4)\n",
    "    return data\n",
    "\n",
    "def create_output(results, image_batch):\n",
    "    final_image= []\n",
    "    blue = (0,0,255) #rgb\n",
    "    for image, (boxes, scores, classes) in zip(image_batch, results):\n",
    "\n",
    "        boxes = tf.cast(boxes, tf.int32)\n",
    "        for bb, score, cl in zip(boxes, scores, classes):\n",
    "            x1y1 = (bb[0], bb[1])\n",
    "            x2y2 = (bb[2], bb[3])\n",
    "            xtext = (bb[0], bb[1]-2)\n",
    "            image = cv2.rectangle(np.array(image), x1y1, x2y2, blue, 1)\n",
    "            image = cv2.putText(image, str(round(score.numpy(), 2)), xtext, cv2.FONT_HERSHEY_SIMPLEX, 0.3, blue, 1)\n",
    "        final_image.append(image)\n",
    "    final_image = tf.stack(final_image)\n",
    "    return final_image\n",
    "\n",
    "def metric(gt_labels, pred_labels, threshold=0.5):\n",
    "    print(np.shape(gt_labels))\n",
    "    \n",
    "    SCALE = 32 #grid, grid, nbb, xywh, mask, class\n",
    "    gt_coords = []\n",
    "    for gt_label in gt_labels:\n",
    "        gt_xy = []\n",
    "        for i in range(3):\n",
    "            for j in range(13):\n",
    "                if gt_label[i][j][0][4] > 0.0:\n",
    "                    center_x = j*SCALE+gt_label[i][j][0][0]*SCALE\n",
    "                    center_y = i*SCALE+gt_label[i][j][0][1]*SCALE\n",
    "\n",
    "                    upper_left = (int(center_x-gt_label[i][j][0][2]*SCALE),int( center_y-gt_label[i][j][0][3]*SCALE))\n",
    "                    under_right = (int(center_x+gt_label[i][j][0][2]*SCALE),int(center_y+gt_label[i][j][0][3]*SCALE))\n",
    "                        \n",
    "                    gt_xy.append([*upper_left, *under_right])\n",
    "\n",
    "                elif gt_label[i][j][1][4] > 0.0:\n",
    "\n",
    "                    center_x = j*SCALE+gt_label[i][j][1][0]*SCALE\n",
    "                    center_y = i*SCALE+gt_label[i][j][1][1]*SCALE\n",
    "\n",
    "                    upper_left = (int(center_x-gt_label[i][j][1][2]*SCALE),int(center_y-gt_label[i][j][1][3]*SCALE))\n",
    "                    under_right = (int(center_x+gt_label[i][j][1][2]*SCALE),int(center_y+gt_label[i][j][1][3]*SCALE))\n",
    "                        \n",
    "                    gt_xy.append([*upper_left, *under_right])\n",
    "            \n",
    "        gt_coords.append(gt_xy)\n",
    "    \n",
    "    total_precision = []\n",
    "    total_recall = []\n",
    "   \n",
    "    for gt_labels, (boxes, scores, classes) in zip(gt_coords, pred_labels): #pro bild      \n",
    "        fn = 0\n",
    "        tp = 0\n",
    "        boxes = tf.cast(boxes, tf.int32)\n",
    "        boxes_copy = boxes.numpy()\n",
    "        \n",
    "        for gt_label in gt_labels:               \n",
    "                \n",
    "            gt_w = abs(gt_label[2] - gt_label[0])\n",
    "            gt_h = abs(gt_label[1]- gt_label[3])\n",
    "            check = False\n",
    "            \n",
    "            best_threshold = 0.5\n",
    "            \n",
    "            for index, bb, in enumerate(boxes_copy): #1 bb pro bild\n",
    "                \n",
    "                w = abs(bb[2] - bb[0])\n",
    "                h = abs(bb[1]-bb[3])\n",
    "\n",
    "                inter_top_left = (max(bb[0], gt_label[0]), max(bb[1], gt_label[1]))\n",
    "                inter_bottom_right = (min(bb[2], gt_label[2]), min(bb[3], gt_label[3]))\n",
    "\n",
    "                intersect_w = max(0, inter_top_left[0]-inter_bottom_right[0])\n",
    "                intersect_h = max(0, inter_top_left[1]-inter_bottom_right[1])\n",
    "                intersect = intersect_w * intersect_h\n",
    "                union = gt_w * gt_h + w*h - intersect\n",
    "\n",
    "                iou = intersect / union\n",
    "\n",
    "                if iou > best_threshold:\n",
    "                    check = True\n",
    "                    best_threshold = iou\n",
    "                    best_index = index\n",
    "                    \n",
    "            if check:\n",
    "                tp +=1\n",
    "                boxes_copy = np.delete(boxes_copy, best_index, 0)\n",
    "            else:\n",
    "                fn +=1\n",
    "                \n",
    "        precision = 0\n",
    "        recall = 0\n",
    "        if len(boxes) !=0:\n",
    "            precision = tp / len(boxes)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        if tp+fn != 0:\n",
    "            recall = tp / (tp + fn)\n",
    "        else: \n",
    "            recall = 0.0\n",
    "\n",
    "        total_precision.append(precision)\n",
    "        total_recall.append(recall)\n",
    "        \n",
    "    return total_precision, total_recall\n",
    "\n",
    "def augment_image(image, label):\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    noise = tf.random.normal(shape=(96,416,3), mean = 0.0, stddev = std, dtype = tf.float32)\n",
    "    image = image + noise\n",
    "    return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(session_logs_path,epochs,batch_size,lr,iou_threshhold,coeff_bb,coeff_obj,coeff_nonObj,verbose=False):\n",
    "    \n",
    "    #fetch data, preproces...\n",
    "    total_precision = []\n",
    "    total_recall = []\n",
    "    total_ap = 0.0\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    data_images, data_labels = get_data()\n",
    "    data = tf.data.Dataset.from_tensor_slices((data_images, data_labels))\n",
    "    data = data.shuffle(1000)\n",
    "    \n",
    "    # 800 Test images, 3200 training images\n",
    "    test_data = data.take(800)\n",
    "    train_data = data.skip(800)\n",
    "    \n",
    "    test_data = test_data.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    train_data = train_data.batch(batch_size).map(augment_image, num_parallel_calls=AUTOTUNE).prefetch(AUTOTUNE)\n",
    "\n",
    "    # ===============================================\n",
    "    # Build the network\n",
    "\n",
    "    bee_detector = Detector(expansion_factor=expansion_factor, n_bb=number_bb, n_classes=classes, \n",
    "                            iou_thresh=iou_threshhold, coeff_bb=coeff_bb, coeff_obj =coeff_obj,\n",
    "                            coeff_nonObj = coeff_nonObj)\n",
    "\n",
    "    bee_detector.detector.set_anchors([(1,2), (3,4)])\n",
    "    bee_detector.backbone.trainable = False\n",
    "    bee_detector.build((1, 96, 416, 3))\n",
    "    bee_detector.summary()\n",
    "    \n",
    "    # ==========================================\n",
    "    #optimizer, routines...\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(lr)\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=bee_detector)\n",
    "    \n",
    "    # load weights (for inference)\n",
    "    #gewichte = \"s3://sagemaker-studio-2hudhs2eoc/Weights/1e-5-batch8-rms-5-5-05/ckpt-400\"\n",
    "    #status = checkpoint.restore(gewichte).expect_partial()\n",
    "\n",
    "    #latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    #bee_detector.load_weights(latest)\n",
    "\n",
    "    writer = tf.summary.create_file_writer(session_logs_path, max_queue=0)\n",
    "    \n",
    "    #======================================================\n",
    "    # Training, testfunctions for eager mode\n",
    "    \n",
    "    @tf.function  \n",
    "    def train_step(img, labels):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = bee_detector(img, training=True)\n",
    "            print(\"Outputshape: {}\".format(output.shape))\n",
    "\n",
    "            xy_loss, wh_loss, obj_loss, class_loss = bee_detector.loss(labels, output)\n",
    "            loss = tf.reduce_sum(xy_loss+wh_loss+obj_loss+class_loss)\n",
    "\n",
    "        gradient = tape.gradient(loss, bee_detector.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradient, bee_detector.trainable_weights))\n",
    "\n",
    "        return (xy_loss, wh_loss, obj_loss, class_loss, loss), output\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(img, labels):\n",
    "        output = bee_detector(img, training=False)\n",
    "        xy_loss, wh_loss, obj_loss, class_loss = bee_detector.loss(labels, output)\n",
    "        loss = tf.reduce_mean(xy_loss+wh_loss+obj_loss+class_loss)\n",
    "        return (xy_loss, wh_loss, obj_loss, class_loss, loss), output\n",
    "\n",
    "\n",
    "    logging_steps = (1600//batch_size) / 10\n",
    "    s = np.int64(0)\n",
    "    j = np.int64(0)\n",
    "    for e in range(epochs):  # actual training\n",
    "\n",
    "        for image_batch, label_batch in train_data:\n",
    "\n",
    "            processed_batch = tf.keras.applications.mobilenet.preprocess_input(image_batch)\n",
    "\n",
    "            loss, output = train_step(processed_batch, label_batch)\n",
    "            \n",
    "            s += 1\n",
    "            if verbose:\n",
    "                if s % 50 == 0:  # log every 50 training steps\n",
    "\n",
    "                    results = bee_detector.detector.raw2Box(output, iouThresh = 0.5, scoreThresh = 0.5)\n",
    "                    sample = create_output(results, image_batch)[0,...]\n",
    "\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(10, 10))\n",
    "                    ax1.matshow(tf.math.reduce_sum(output[0, :,:,:,4], axis=-1))\n",
    "                    ax1.set_title('prediction')\n",
    "                    ax1.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "                    ax2.matshow(tf.math.reduce_sum(label_batch[0, :,:,:,4], axis=-1)) \n",
    "                    ax2.set_title('mask')\n",
    "                    ax2.xaxis.set_ticks_position('bottom')\n",
    "                    \n",
    "                    ax3.imshow(image_batch[0,:,:,:]) \n",
    "                    ax3.set_title('image')\n",
    "                    \n",
    "                    ax4.imshow(sample)\n",
    "                    ax4.set_title(\"output\")\n",
    "\n",
    "                    f.tight_layout()\n",
    "                    plt.show()\n",
    "\n",
    "            if s % 50 == 0:\n",
    "\n",
    "                results = bee_detector.detector.raw2Box(output, iouThresh = 0.3, scoreThresh= 0.3)\n",
    "                final_image = create_output(results, image_batch)\n",
    "\n",
    "                with writer.as_default():\n",
    "                    print(\"Epoch: {}  Step: {}  Loss: {}\".format(e, s, loss[4]))\n",
    "                    summary.scalar(\"train/xy-loss\", tf.reduce_mean(loss[0]),step=s)\n",
    "                    summary.scalar(\"train/wh-loss\", tf.reduce_mean(loss[1]), step=s)\n",
    "                    summary.scalar(\"train/obj-loss\", tf.reduce_mean(loss[2]), step=s)\n",
    "                    summary.scalar(\"train/class-loss\", tf.reduce_mean(loss[3]), step=s)\n",
    "                    summary.scalar(\"train/total-loss\", loss[4], step=s)\n",
    "                    summary.image(\"train/output-image\", final_image, max_outputs=3, step=s)\n",
    "                    summary.image(\"train/mask\",tf.math.reduce_sum(label_batch[...,4],axis = -1,keepdims=True), step=s)\n",
    "                    summary.image(\"train/predict\",(tf.math.reduce_sum(output[...,4],axis = -1,keepdims=True).numpy()*255).astype(np.uint8), step=s)\n",
    "                    writer.flush()\n",
    "        \n",
    "        #=======================================================\n",
    "        # test routine\n",
    "        for test_image_batch, test_label_batch in test_data:\n",
    "            \n",
    "            processed_test_batch = tf.keras.applications.mobilenet.preprocess_input(test_image_batch)\n",
    "            test_loss, test_output = test_step(processed_test_batch,  test_label_batch)\n",
    "            j += 1\n",
    "            \n",
    "            \n",
    "            if j % 50 == 0: \n",
    "                \n",
    "                test_results = bee_detector.detector.raw2Box(test_output, scoreThresh = 0.3, iouThresh = 0.3)\n",
    "                precision, recall = metric(test_label_batch, test_results)\n",
    "                \n",
    "                total_precision.append(precision)\n",
    "                total_recall.append(recall)\n",
    "                \n",
    "                test_results = bee_detector.detector.raw2Box(test_output, scoreThresh = 0.3, iouThresh = 0.3)\n",
    "                test_final_image = create_output(test_results, test_image_batch)\n",
    "                \n",
    "                with writer.as_default():\n",
    "                    print(\"Epoch: {}  Step: {}  Test-Loss: {}\".format(e, j, test_loss[4]))\n",
    "                    summary.scalar(\"test/xy-loss\", tf.reduce_mean(test_loss[0]),step=j)\n",
    "                    summary.scalar(\"test/wh-loss\", tf.reduce_mean(test_loss[1]), step=j)\n",
    "                    summary.scalar(\"test/obj-loss\", tf.reduce_mean(test_loss[2]), step=j)\n",
    "                    summary.scalar(\"test/class-loss\", tf.reduce_mean(test_loss[3]), step=j)\n",
    "                    summary.scalar(\"test/total-loss\", test_loss[4], step=j)\n",
    "                    summary.image(\"test/output-image\", test_final_image, max_outputs=3, step=j)\n",
    "                    summary.image(\"test/mask\",tf.math.reduce_sum(test_label_batch[...,4],axis = -1,keepdims=True), step=j)\n",
    "                    summary.image(\"test/predict\",(tf.math.reduce_sum(test_output[...,4],axis = -1,keepdims=True).numpy()*255).astype(np.uint8), step=j)\n",
    "                    writer.flush()\n",
    "\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "        \n",
    "    total_precision = [prec for sub_list in total_precision for prec in sub_list]\n",
    "    total_recall = [rec for sub_list in total_recall for rec in sub_list]\n",
    "\n",
    "    total_precision = sum(total_precision) / (800/batch_size)\n",
    "    total_recall = sum(total_recall) / (800/batch_size)\n",
    "    print(\"Average-total_precision:\", total_precision)\n",
    "    print(\"Average-total_recall:\", total_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Set Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Related\n",
    "epochs = int(400)  \n",
    "batch_size = 8\n",
    "lr = 1e-5\n",
    "\n",
    "# Loss Related\n",
    "iou_threshhold = 0.5\n",
    "coeff_bb = 5\n",
    "coeff_obj = 5\n",
    "coeff_nonObj = 0.5\n",
    "\n",
    "#noise\n",
    "std = 0.0001\n",
    "\n",
    "# Architecture Related\n",
    "expansion_factor = 6\n",
    "number_bb = 2\n",
    "classes = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "import tensorflow.summary as summary\n",
    "\n",
    "logs_path = \"s3://sagemaker-studio-2hudhs2eoc/Logs\"\n",
    "\n",
    "checkpoint_directory = \"s3://sagemaker-studio-2hudhs2eoc/Checkpoints\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Describe the session:  ggd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s1612477773: ggd\n",
      "read files: 0\n",
      "read files: 100\n",
      "read files: 200\n",
      "read files: 300\n",
      "read files: 400\n",
      "read files: 500\n",
      "read files: 600\n",
      "read files: 700\n",
      "read files: 800\n",
      "read files: 900\n",
      "read files: 1000\n",
      "read files: 1100\n",
      "read files: 1200\n",
      "read files: 1300\n",
      "read files: 1400\n",
      "read files: 1500\n",
      "read files: 1600\n",
      "read files: 1700\n",
      "read files: 1800\n",
      "read files: 1900\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Model: \"detector\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Functi (None, 3, 13, 1280)       2257984   \n",
      "_________________________________________________________________\n",
      "yolovx (YOLOVX)              multiple                  17733132  \n",
      "_________________________________________________________________\n",
      "model (Functional)           (None, 12, 52, 192)       65920     \n",
      "_________________________________________________________________\n",
      "model_1 (Functional)         (None, 6, 26, 576)        616256    \n",
      "=================================================================\n",
      "Total params: 19,991,116\n",
      "Trainable params: 17,718,796\n",
      "Non-trainable params: 2,272,320\n",
      "_________________________________________________________________\n",
      "Outputshape: (8, 3, 13, 2, 6)\n",
      "Outputshape: (8, 3, 13, 2, 6)\n",
      "Epoch: 0  Step: 50  Loss: 198.61712646484375\n",
      "Epoch: 0  Step: 100  Loss: 203.55612182617188\n",
      "Epoch: 0  Step: 150  Loss: 217.12127685546875\n",
      "Epoch: 0  Step: 200  Loss: 156.79165649414062\n",
      "Epoch: 0  Step: 250  Loss: 175.58934020996094\n",
      "Epoch: 0  Step: 300  Loss: 167.98619079589844\n",
      "Epoch: 0  Step: 350  Loss: 202.8568115234375\n",
      "Epoch: 0  Step: 400  Loss: 140.95718383789062\n",
      "(8, 3, 13, 2, 6)\n",
      "Epoch: 0  Step: 50  Test-Loss: 15.475275039672852\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4abf5fb51b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msession_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msession_logs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_logs_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miou_threshhold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeff_bb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeff_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeff_nonObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-aeec276de616>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(session_logs_path, epochs, batch_size, lr, iou_threshhold, coeff_bb, coeff_obj, coeff_nonObj, verbose)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/predict\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/average-precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ap' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    session_name = get_session_name()\n",
    "    session_logs_path = os.path.join(logs_path, session_name)\n",
    "    train(session_logs_path,epochs,batch_size,lr,iou_threshhold,coeff_bb,coeff_obj,coeff_nonObj, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/mxnet-1.8-gpu-py37-cu110-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
